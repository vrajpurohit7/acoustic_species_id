<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Acoustic Species ID</title>
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
</head>
<body class="vsc-initialized">
    <!-- Header -->
    <header class="w3-display-container w3-content w3-center" style="max-width:100%">
        <img class="w3-image" src="./images/home_page.jpg" alt="HomeImg" style="width:100%; height: auto;">
        <div class="w3-display-middle w3-padding-large w3-border w3-wide w3-text-light-grey w3-center">
            <h1 class="w3-xxlarge">ACOUSTIC SPECIES ID</h1>
        </div>
    </header>
    
    <!-- Page content -->
    <div class="w3-content w3-padding-large w3-margin-top" id="portfolio" style="display: flex; flex-direction: column; text-align: justify;">
        <!-- Intro Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="intro">
            <h2>Welcome to the Acoustic Species Identification Project</h2>
            <p>This project focuses on classifying bird vocalizations and extends previous work from Engineers for Exploration (E4E). We are developing techniques to overcome challenges in audio data classification due to domain shift from labeled training data to unlabeled, noisy field recordings.</p>
            <p>Our objective is to leverage modern machine learning architectures and domain-specific adaptations to accurately identify bird species from acoustic data. This work is part of our broader efforts to aid ecological research and conservation through advanced bioacoustic classification systems.</p>
            <p>Currently, we are working on implementing and evaluating various machine learning models, including CNNs and newer RNN architectures, to improve the accuracy and reliability of our identification systems. Our work contributes to the BirdCLEF 2024 competition on Kaggle, aiming to push the boundaries of current bioacoustic research.</p>
        </div>

        <!-- Architecture -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="architecture">
            <h2> Architecture Exploration </h2>
            <p>
                The standard pipeline for audio classification is convolutional; an image is first converted into a spectrogram, which is then passed through a standard convolutional architecture like EfficientNet.
                However, recently a new class of state-space models have been shown to be effective in audio classification tasks.
                We explored the potential application of such models to bird call classification, integrating them into our existing pipeline to compare their performance in a controlled environment.
            </p>
            <p>
                We chose the <a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba architecture</a> as the backbone for our method, owing to its relative success on modeling audio data.
                One initial challenge was that the version of Mamba compatible with audio data -- a mixture of Mamba and
                <a href="https://arxiv.org/abs/2111.00396" target="_blank">S4</a> in a
                <a href="https://arxiv.org/abs/2202.09729" target="_blank">Sashimi architecture</a>
                -- has not been publicly implemented.
                Splicing together code, we obtained a model that successfully began to train but was unusably slow in training, likely due to backpropagation through time for the time-varying Mamba component.
                The Mamba model could train at roughly 1.5 clips per second, compared to 180 from EfficientNet.
                After training for 5 days on a 24-GB GPU, the model had trained for only two epochs and achieved validation mAP of roughly 0.1 -- an improvement over its 0.05 to start with, but nowhere near the 0.5 to 0.7 range of EfficientNet trained on many more epochs.
                At this point, we determined that without significant optimization, the model would not be able to compete with our existing pipeline.
            </p>
            <p>
                We found more success in adapting <a href="https://arxiv.org/abs/2405.11831" target="_blank">SSAMBA</a>, a Mamba-based model operating on spectrograms which is pretrained in such a way as to adapt quickly to classification tasks.
                The SSAMBA work found success in classification on the <a href="https://github.com/karolpiczak/ESC-5" target="_blank">ESC dataset</a>, which includes some categories of bird calls, and so it seemed to be a natural fit for our task.
                Such a pretrained model would hopefully be more data-efficient than our existing pipeline, which is trained from scratch and thus begins with no acoustic pattern recognition.
            </p>
            <p>
                We integrated SSAMBA into our pipeline, utilizing the same data augmentation and training setups as the EfficientNet models and the optimization parameters from the SSAMBA paper.
                The model is still significantly less efficient in training than EfficientNet, at a rate of 7 to 8 clips per second.
                Due to time constraints, all comparisons are made at 2 epochs.
                This is not entirely fair, as EfficientNet is trained for 50 epochs, but it is a reasonable comparison given the ability to train SSAMBA.

                We find that SSAMBA-small and SSAMBA-base both achieve a higher train mAP than EfficientNet, exceeding 0.30 when EFC is only 0.27 (SSAMBA-tiny is still training).
                However, the validation mAP is lower for both SSAMBA models, indicating that they are overfitting.

                Given that SSAMBA-small has only 26M parameters to EFC's 39M, and that SSAMBA-tiny is on a trajectory to win in training mAP as well, it is interesting to see that SSAMBA is sufficiently expressive to overfit to such a complicated distribution.
                This is encouraging, with the potential that small SSAMBA models with proper regularization and more aggressive data augmentation could be competitive in this domain.
            </p>
            
        </div>
        
        <!-- Ludwig's part -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="ensemble">
            <h2>Ensemble Learning Strategy</h2>
            <p>We use ensemble learning methods to enhance the accuracy of our bird species identification project, which is part of the BirdCLEF competition focusing on identifying bird sounds from complex audio environments. This competition requires analyzing diverse audio data, including noisy field recordings with overlapping bird calls. Our approach combines several advanced machine learning models, each trained independently, to better predict bird species. Specifically, we utilized models like <code>tf_efficientnet_b4</code> and <code>resnetv2_101</code>, achieving our highest score of 0.66 with the latter after 50 training epochs. Other models like <code>eca_nfnet_l0</code> and <code>mobilenetv3_large_100_miil_in21k</code> scored 0.62 and 0.60, respectively, demonstrating robust performance across varied audio datasets. We are also currently evaluating an enhanced ensemble strategy that combines two powerful models, <code>mobilenetv3_large_100_miil_in21k</code> and <code>resnetv2_50</code>, using an optimization tool called ONNX to further improve processing efficiency and model performance. Together, <code>mobilenet</code> and <code>resnetv2_50</code> achieved a combined score of 0.59.</p>
            <p>However, not all attempts were successful; models such as <code>seresnext101_32x4d</code> and <code>eca_nfnet_l2</code> timed out, likely due to their large size which made processing within Kaggle's constraints challenging. Participating in Kaggle proved difficult due to several limitations, including no internet access during submission, an unorganized file system, and the lack of GPU usage, which significantly hindered our ability to utilize more computationally intensive models effectively.</p>
            <p>Despite these challenges, our team achieved an impressive 80/780, earning a bronze medal in the competition. This accomplishment underscores the effectiveness of our ensemble strategy in handling the complex task of bird species identification from audio data, even within the restrictive environment of a competitive Kaggle contest.</p>
        </div>
        
        <!-- Team Members Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="contact">
            <!-- <h2>Contact Us</h2>
            <p>If you have any questions or would like to know more about our project, please feel free to contact us at <a href="mailto:info@acousticspeciesid.com">info@acousticspeciesid.com</a>.</p> -->
    
            <h5><b>Team Members</b></h5>
            <ul>
              <li>Ludwig von Schoenfeldt</li>
              <li>Sean O'Brien</li>
              <li>Vibhuti Rajpurohit</li>
              <li>Geelon So</li>
            </ul>
        </div>
        <!-- Documents Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="docs">
          <h2>Project Documents</h2>
          <p>Explore detailed reports and documents related to our project:</p>
          <ul>
              <li><a href="./docs/Project_Specification.pdf" target="_blank">Project Specification</a></li>
              <li><a href="./docs/Milestone_Report.pdf" target="_blank">Milestone Report</a></li>
          </ul>
        </div>
    </div>
    
    <!-- Footer -->
    <footer class="w3-center w3-light-grey w3-padding-32">
        <p>This website has been hosted by UCSD students as part of coursework in CSE 145. </p>
    </footer>
</body>
</html>
