<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Acoustic Species ID</title>
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <!-- <link rel="stylesheet" href="styles.css"> -->
</head>
<body class="vsc-initialized">

    <!-- Header -->
    <header class="w3-display-container w3-content w3-center" style="max-width:100%">
        <img class="w3-image" src="./images/home_page.jpg" alt="HomeImg" style="width:100%; height: auto;">
        <div class="w3-display-middle w3-padding-large w3-border w3-wide w3-text-light-grey w3-center">
            <h1 class="w3-xxlarge">ACOUSTIC SPECIES ID</h1>
        </div>
    </header>
    
    
    <!-- Navbar on small screens -->
    <div class="w3-center w3-light-grey w3-padding-16 w3-hide-large w3-hide-medium">
    </div>
    <!-- Page content -->
    <!-- <img src="./images/3.jpeg" alt="Left Image" class="w3-left w3-margin-right" style="width:200px;height:200px; margin-top: 300px;margin-left: 30px;"> -->
      
      <!-- Right image -->
      <!-- <img src="./images/2.jpeg" alt="Right Image" class="w3-right w3-margin-left" style="width:200px;height:200px; margin-top: 800px;margin-right: 30px;"> -->
    <div class="w3-content w3-padding-large w3-margin-top" id="portfolio" style="display: flex; flex-direction: column; text-align: justify;">
      
      
      <!-- Contact -->
      <!-- 
        intro: what the project is about from the previous reports
        ludwig's: the ensamble work (use the Milestone Reports)
        shawn: his mamba implementation
        geelon: 
        also post all of the materials that you have to generate for this class in the repo
       -->
      <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="contact">
        <p>Birds can serve as effective indicators of changes in biodiversity due to their mobility and diverse habitat needs. The alterations in the species composition and bird population can reflect the progress or shortcomings of restoration projects. However, conventional bird biodiversity surveys relying on observer-based methods are often difficult and expensive to conduct over extensive areas. Alternatively, using audio recording devices combined with modern analytical techniques that employ machine learning can enable conservationists to study the correlation between restoration interventions and biodiversity in greater depth by sampling larger spatial scales with higher temporal resolution. Ultimately, the optimal objective is to create a pipeline capable of precisely detecting a wide range of species vocalizations within a specified location where audio equipment is installed.</p>
    
        <p>Our team's MVP was to design and implement a solution for analyzing audio data to identify different bird species based on their distinct calls. By utilizing machine learning techniques, we aim to create a powerful model that can accurately recognize and predict the species of birds present in the audio recordings. We successfully accomplished this.</p>
    
        <p>The proposed end product of this project is a robust and efficient multi-species classifier that can seamlessly process audio data and identify the bird species present in the recordings. Our solution will use melspectogram to represent features from the audio data, which will then be used to train the machine learning model. Our work is available in <a href="https://github.com/shivanihariprasad/acoustic-species-identification">this repository</a>, where you can view our project.</p>
    
        <h5><b>Data Set:</b></h5>
        <p>The train data contains 264 species from Kenya, Africa, and the test set consists of 191 10-minute soundscapes. Xeno-canto provided 16,900 audio recordings which can be used to train a classifier. This data is part of the <a href="https://www.kaggle.com/competitions/birdclef-2023">BirdClef</a> 2023 Kaggle competition.</p>
    
        <p><b>PyHa</b>: A tool designed to convert audio-based "weak" labels to "strong" moment-to-moment labels. We are using the TweetyNet model variant to identify bird calls in the audio clip. The link to this tool can be found here <a href="https://github.com/UCSD-E4E/PyHa">PyHa</a>.</p>
    
        <p>This package is being developed and maintained by the Engineers for Exploration Acoustic Species Identification Team in collaboration with the San Diego Zoo Wildlife Alliance.</p>
    
        <b>Step 1:</b> Audio data is sent to PyHA, a platform capable of processing audio signals, in order to obtain a set of 5-second audio segments. These segments will be analyzed to determine the presence or absence of bird sounds, and data frames will be generated.
        <br><br>
        <b>Step 2:</b> To visualize and analyze this data, we utilized the librosa library, which offers a range of audio processing functionalities. In particular, we employed librosa to generate melspectrograms for the data frames.
        <br><br>
        <b>Step 3:</b> To initiate our analysis, we adopted a random selection process to choose 30 classes from the available data. The purpose of this selection was to generate melspectrogram images for the corresponding data frames derived from the PyHa output. By selecting a diverse set of 30 classes, we aimed to obtain a representative sample of the dataset, encompassing various bird species and vocalizations.
    
        <p>The models we chose were: VGG16, EfficientNet Model - B0, B3, B7, Resnet50.</p>
    
        <p><b>VGG16:</b> The validation accuracy achieved on the 30 classes was 89.06%. The macro average precision was determined to be 78.2%.</p>
    
        <p><b>EfficientNet:</b> The validation accuracy achieved on the 30 classes for the EfficientNet B0 model was 74.35%, for the EfficientNet B3 model was 64.36%, and for the EfficientNet B7 model was 38.01%. The macro average precision was determined to be 78.2% for the EfficientNet B0 model, 74.60% for the EfficientNet B3 model, and 44.06% for the EfficientNet B7 model.</p>
    
        <p><b>Resnet50:</b> The validation accuracy achieved was only 36%.</p>
    
        <h5><b>Data Augmentation Techniques</b></h5>
        <ul>
          <li>Noise Reduction: We employed noise reduction techniques to minimize background noise and enhance the clarity of the melspectrogram images. By reducing unwanted noise, we aimed to improve the model's ability to focus on relevant audio patterns and features. We achieved a validation accuracy of 83.19%.</li>
          <li>Time Masking: Time masking involves masking consecutive time steps in the melspectrogram image, effectively introducing temporal gaps. This technique helps the model generalize better and be less reliant on specific temporal dependencies in the training data.</li>
          <li>Frequency Masking: Similar to time masking, frequency masking involves masking consecutive frequency bins in the melspectrogram image. This technique encourages the model to focus on different frequency components and reduces overfitting to specific frequency patterns.</li>
          <li>Time Stretch: Time stretching is a common technique used in audio data augmentation to alter the duration of an audio signal without changing its pitch. It can be useful for tasks such as speech recognition, audio classification, and music analysis. We randomized time masking, time stretch, and frequency masking among the dataset during training and validation and achieved a validation accuracy of 94.62%.</li>
          <li>Pitch Shift: By applying pitch shifting, we altered the pitch or tonal characteristics of the audio signals. This transformation simulates variations in pitch that can occur naturally in different recordings or environments. It allows the model to learn robust representations that are invariant to slight pitch variations. We achieved a validation accuracy of 81.59%.</li>
          <li>Gaussian Noise: Adding Gaussian noise to the melspectrogram images introduced random variations that mimic real-world recording conditions. By doing so, we aimed to improve the model's resilience to noise and enhance its ability to generalize well in the presence of unexpected acoustic variations. We achieved a validation accuracy of 85.59%.</li>
        </ul>
        <h5><b>Team Members</b></h5>
        <ul>
          <li>Ludwig von Schoenfeldt</li>
          <li>Sean O'Brien</li>
          <li>Vibhuti Rajpurohit</li>
          <li>Geelon So</li>
        </ul>
      </div>
    </div>
    
    <!-- Footer -->
    <footer class="w3-center w3-light-grey w3-padding-32">
      <p>This website has been hosted by UCSD students as part of coursework in CSE 145. </p>
    </footer>
    
    
    
</body>

</html>
