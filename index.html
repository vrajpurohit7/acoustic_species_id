<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Acoustic Species ID</title>
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
</head>
<body class="vsc-initialized">
    <!-- Header -->
    <header class="w3-display-container w3-content w3-center" style="max-width:100%">
        <img class="w3-image" src="./images/home_page.jpg" alt="HomeImg" style="width:100%; height: auto;">
        <div class="w3-display-middle w3-padding-large w3-border w3-wide w3-text-light-grey w3-center">
            <h1 class="w3-xxlarge">ACOUSTIC SPECIES ID</h1>
        </div>
    </header>
    
    <!-- Page content -->
    <div class="w3-content w3-padding-large w3-margin-top" id="portfolio" style="display: flex; flex-direction: column; text-align: justify;">
        <!-- Intro Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="intro">
            <h2>Welcome to the Acoustic Species Identification Project</h2>
            <p>This project focuses on classifying bird vocalizations and extends previous work from Engineers for Exploration (E4E). We are developing techniques to overcome challenges in audio data classification due to domain shift from labeled training data to unlabeled, noisy field recordings.</p>
            <p>Our objective is to leverage modern machine learning architectures and domain-specific adaptations to accurately identify bird species from acoustic data. This work is part of our broader efforts to aid ecological research and conservation through advanced bioacoustic classification systems.</p>
            <p>Currently, we are working on implementing and evaluating various machine learning models, including CNNs and newer RNN architectures, to improve the accuracy and reliability of our identification systems. Our work contributes to the BirdCLEF 2024 competition on Kaggle, aiming to push the boundaries of current bioacoustic research.</p>
        </div>

        <!-- Architecture -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="architecture">
            <h2> Architecture Exploration </h2>
            <p>
                The standard pipeline for audio classification is convolutional; an image is first converted into a spectrogram, which is then passed through a standard convolutional architecture like EfficientNet.
                However, recently a new class of state-space models have been shown to be effective in audio classification tasks.
                We explored the potential application of such models to bird call classification, integrating them into our existing pipeline to compare their performance in a controlled environment.
            </p>
            <p>
                We chose the <a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba architecture</a> as the backbone for our method, owing to its relative success on modeling audio data.
                One initial challenge was that the version of Mamba compatible with audio data -- a mixture of Mamba and
                <a href="https://arxiv.org/abs/2111.00396" target="_blank">S4</a> in a
                <a href="https://arxiv.org/abs/2202.09729" target="_blank">Sashimi architecture</a>
                -- has not been publicly implemented.
                Splicing together code, we obtained a model that successfully began to train but was unusably slow in training, likely due to backpropagation through time for the time-varying Mamba component.
                The Mamba model could train at roughly 1.5 clips per second, compared to 180 from EfficientNet.
                After training for 5 days on a 24-GB GPU, the model had trained for only two epochs and achieved validation mAP of roughly 0.1 -- an improvement over its 0.05 to start with, but nowhere near the 0.5 to 0.7 range of EfficientNet trained on many more epochs.
                At this point, we determined that without significant optimization, the model would not be able to compete with our existing pipeline.
            </p>
            <p>
                We found more success in adapting <a href="https://arxiv.org/abs/2405.11831" target="_blank">SSAMBA</a>, a Mamba-based model operating on spectrograms which is pretrained in such a way as to adapt quickly to classification tasks.
                The SSAMBA work found success in classification on the <a href="https://github.com/karolpiczak/ESC-5" target="_blank">ESC dataset</a>, which includes some categories of bird calls, and so it seemed to be a natural fit for our task.
                Such a pretrained model would hopefully be more data-efficient than our existing pipeline, which is trained from scratch and thus begins with no acoustic pattern recognition.
            </p>
            <p>
                We integrated SSAMBA into our pipeline, utilizing the same data augmentation and training setups as the EfficientNet models and the optimization parameters from the SSAMBA paper.
                The model is still significantly less efficient in training than EfficientNet, at a rate of 7 to 8 clips per second.
                Due to time constraints, all comparisons are made at 2 epochs.
                This is not entirely fair, as EfficientNet is trained for 50 epochs, but it is a reasonable comparison given the ability to train SSAMBA.

                We find that SSAMBA-small and SSAMBA-base both achieve a higher train mAP than EfficientNet, exceeding 0.30 when EFC is only 0.27 (SSAMBA-tiny is still training).
                However, the validation mAP is lower for both SSAMBA models, indicating that they are overfitting.

                Given that SSAMBA-small has only 26M parameters to EFC's 39M, and that SSAMBA-tiny is on a trajectory to win in training mAP as well, it is interesting to see that SSAMBA is sufficiently expressive to overfit to such a complicated distribution.
                This is encouraging, with the potential that small SSAMBA models with proper regularization and more aggressive data augmentation could be competitive in this domain.
            </p>
            
        </div>

        
        <!-- Representation -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="representation">
            <h2> Representation Exploration </h2>
            <p>
                <strong>Background.</strong>
                In addition to exploring additional architectures, we also 
                consider alternative representations of audio data. As noted
                above, a common approach is to compute the spectrogram of an 
                audio clip before treating it as an RGB image. In particular,
                this means learning using architectures designed for natural 
                images. Whereas these architectures have been seen to be able
                to recover perceptually salient features such as edges and 
                textures in natural images, it is less clear whether analogous
                features should be as prominent in spectrograms.
            </p>

            <p>
                <strong>Prior work.</strong>
                We adapted the <em>expand-and-sparsify</em> representation, 
                which was introduced by <a href="https://arxiv.org/abs/2006.03741">Dasgupta and Tosh (2020)</a>.
                In their paper, they study a simplified neural architecture 
                found initially found in the <a href="https://www.science.org/doi/full/10.1126/science.aam9868">fruit fly's olfactory system</a>. 
                In the fly brain, representations of olfactory stimuli are 
                constructed as follows: a fly has around 50 types of receptors 
                that can detect different components of odors. When exposed to
                a stimulus, the corresponding activations generate an initial
                <em>dense</em> representation of the odor, which we can model 
                as a vector in 50 dimensions. In the <em>expand</em> step, this
                initial representation is randomly projected into much higher 
                dimensions. This is followed by a <em>sparsify</em> step, where 
                only the top-<em>k</em> components are kept, so that this leads 
                to a <em>k</em>-sparse high-dimensional representation. It was 
                shown that such a representation can disentangle non-linear
                features, so that it becomes possible to learn by simply 
                training a last linear layer. Here, we ask whether such a 
                representation can be beneficial to acoustic data as well.
            </p>

            <p>
                <strong>Our work.</strong>
                In order to adapt this method to time series data, we 
                implemented a convolutional version of the expand-and-sparsify 
                architecture. To briefly describe this method, suppose we have
                a Mel spectrogram, which is a multi-dimensional time series.
                That is, it is an array of size <code>(n_mels, duration)</code>. 
                Then, we can construct a kernel of size 
                <code>(n_output, n_mels, window)</code> which scans along the 
                spectrogram (here the length of <code>window</code> is smaller 
                than the audio clip's <code>duration</code>). After taking the 
                convolution, we obtain a new time series of with dimension
                 <code>n_output</code>. This can be passed to downstream models.
            </p>

            <p>
                <strong>Experimental method.</strong>
                To evaluate this method, we built a small residual network 
                for time series data. We can think of a time series object as 
                having two axes: a spatial axis and a temporal axis. Consider 
                a <code>d</code>-dimensional time series of length 
                <code>T</code>, which is an array of shape <code>(d,T)</code>.
                We design the resnet to have convolutional kernels that span all 
                spatial dimensions at once, so that its shape is 
                <code>(d, T_window)</code>. We trained two such resnets: (a) the 
                baseline takes the Mel spectrogram directly, while (b) the 
                experimental version takes the expand-and-sparsify 
                representation.
            </p>

            <p>
                <strong>Results.</strong>
                While the hope was that the expand-and-sparsify representation
                can speed up training by disentagling non-linear features, it
                does not appear to make learning easier. We submitted these two
                models after 2 epochs of training to the BirdCLEF competition. 
                
                The baseline (a) received a score of 0.58, while the 
                experimental version (b) received a score of 0.48, performing no 
                better than random. Therefore, it seems that in fact, the choice
                of parameters led to an increase of difficulty in learning.
            </p>

            <p>
                <strong>Additional discussion.</strong>
                We continued training the baseline (a) for 40 epochs, and it 
                receives a slightly improved score of 0.60 on BirdCLEF. On the 
                one hand, this performs worse than an comparably-trained 
                EfficientNet. However, the this baseline (a) is smaller in size
                (48mb vs 74mb), and it runs much faster on CPU only, as required
                by the BirdCLEF competition (20min vs 80min).
                
                This suggests that it may be worth exploring additional resnet 
                architectures that treat the spectrogram as a time series rather
                than a natural image. 

                Finally, the poor results for the expand-and-sparsify 
                representation do not necessarily rule it out; a more robust 
                set of experiments is required for this, and there are further
                modifications suggested in the original paper that we can also 
                try.
            </p>
        </div>
        
        <!-- Ludwig's part -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="ensemble">
            <h2>Ensemble Learning Strategy</h2>
            <p>We use ensemble learning methods to enhance the accuracy of our bird species identification project, which is part of the BirdCLEF competition focusing on identifying bird sounds from complex audio environments. This competition requires analyzing diverse audio data, including noisy field recordings with overlapping bird calls. Our approach combines several advanced machine learning models, each trained independently, to better predict bird species. Specifically, we utilized models like <code>tf_efficientnet_b4</code> and <code>resnetv2_101</code>, achieving our highest score of 0.66 with the latter after 50 training epochs. Other models like <code>eca_nfnet_l0</code> and <code>mobilenetv3_large_100_miil_in21k</code> scored 0.62 and 0.60, respectively, demonstrating robust performance across varied audio datasets. We are also currently evaluating an enhanced ensemble strategy that combines two powerful models, <code>mobilenetv3_large_100_miil_in21k</code> and <code>resnetv2_50</code>, using an optimization tool called ONNX to further improve processing efficiency and model performance. Together, <code>mobilenet</code> and <code>resnetv2_50</code> achieved a combined score of 0.59.</p>
            <p>However, not all attempts were successful; models such as <code>seresnext101_32x4d</code> and <code>eca_nfnet_l2</code> timed out, likely due to their large size which made processing within Kaggle's constraints challenging. Participating in Kaggle proved difficult due to several limitations, including no internet access during submission, an unorganized file system, and the lack of GPU usage, which significantly hindered our ability to utilize more computationally intensive models effectively.</p>
            <p>Despite these challenges, our team achieved an impressive 80/780, earning a bronze medal in the competition. This accomplishment underscores the effectiveness of our ensemble strategy in handling the complex task of bird species identification from audio data, even within the restrictive environment of a competitive Kaggle contest.</p>
        </div>
        
        <!-- Team Members Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="contact">
            <!-- <h2>Contact Us</h2>
            <p>If you have any questions or would like to know more about our project, please feel free to contact us at <a href="mailto:info@acousticspeciesid.com">info@acousticspeciesid.com</a>.</p> -->
    
            <h5><b>Team Members</b></h5>
            <ul>
              <li>Ludwig von Schoenfeldt</li>
              <li>Sean O'Brien</li>
              <li>Vibhuti Rajpurohit</li>
              <li>Geelon So</li>
            </ul>
        </div>
        <!-- Documents Section -->
        <div class="w3-light-grey w3-padding-large w3-padding-32 w3-margin-top" id="docs">
          <h2>Project Documents</h2>
          <p>Explore detailed reports and documents related to our project:</p>
          <ul>
              <li><a href="./docs/Project_Specification.pdf" target="_blank">Project Specification</a></li>
              <li><a href="./docs/Milestone_Report.pdf" target="_blank">Milestone Report</a></li>
          </ul>
        </div>
    </div>
    
    <!-- Footer -->
    <footer class="w3-center w3-light-grey w3-padding-32">
        <p>This website has been hosted by UCSD students as part of coursework in CSE 145. </p>
    </footer>
</body>
</html>
